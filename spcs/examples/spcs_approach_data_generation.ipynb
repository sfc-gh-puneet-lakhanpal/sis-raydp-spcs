{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Data Generation for Spark MLLib based Model Training on Snowpark Container Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-08 02:47:38,686\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray version: 2.42.0\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import pprint\n",
    "import warnings\n",
    "import logging    \n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import socket\n",
    "import snowflake.connector\n",
    "from snowflake.snowpark import Session\n",
    "print(f\"Ray version: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Snowpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection() -> snowflake.connector.SnowflakeConnection:\n",
    "    if os.path.isfile(\"/snowflake/session/token\"):\n",
    "        creds = {\n",
    "            'host': os.getenv('SNOWFLAKE_HOST'),\n",
    "            'port': os.getenv('SNOWFLAKE_PORT'),\n",
    "            'protocol': \"https\",\n",
    "            'account': os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "            'authenticator': \"oauth\",\n",
    "            'token': open('/snowflake/session/token', 'r').read(),\n",
    "            'warehouse': \"LARGE_WH\",\n",
    "            'database': os.getenv('SNOWFLAKE_DATABASE'),\n",
    "            'schema': os.getenv('SNOWFLAKE_SCHEMA'),\n",
    "            'client_session_keep_alive': True,\n",
    "            'check_arrow_conversion_error_on_every_column': False\n",
    "        }\n",
    "    else:\n",
    "        creds = {\n",
    "            'account': os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "            'user': os.getenv('SNOWFLAKE_USER'),\n",
    "            'password': os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "            'warehouse': snowflake_warehouse,\n",
    "            'database': os.getenv('SNOWFLAKE_DATABASE'),\n",
    "            'schema': os.getenv('SNOWFLAKE_SCHEMA'),\n",
    "            'client_session_keep_alive': True,\n",
    "            'check_arrow_conversion_error_on_every_column': False\n",
    "        }\n",
    "\n",
    "    connection = snowflake.connector.connect(**creds)\n",
    "    return connection\n",
    "\n",
    "def get_session() -> Session:\n",
    "    return Session.builder.configs({\"connection\": connection()}).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"RAYDP_SIS_DB\"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.get_current_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 02:47:39,607\tINFO worker.py:1654 -- Connecting to existing Ray cluster at address: 10.244.24.9:6379...\n",
      "2025-07-08 02:47:39,616\tINFO worker.py:1832 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m10.244.24.9:8265 \u001b[39m\u001b[22m\n",
      "[2025-07-08 02:47:39,620 I 25445 25445] logging.cc:293: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to 1\n"
     ]
    }
   ],
   "source": [
    "cli = ray.init(address=\"auto\", ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_resources = ray.cluster_resources()\n",
    "nodes = ray.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cluster Information:\n",
      "   Node 1: {'node:10.244.24.137': 1.0, 'CPU': 28.0, 'object_store_memory': 11220602060.0, 'memory': 246362374963.0}\n",
      "   Node 2: {'node:__internal_head__': 1.0, 'CPU': 28.0, 'object_store_memory': 11220602060.0, 'memory': 235982446387.0, 'node:10.244.24.9': 1.0}\n",
      "   Node 3: {'node:10.244.24.73': 1.0, 'CPU': 28.0, 'object_store_memory': 11220602060.0, 'memory': 246362051379.0}\n",
      "   Node 4: {'object_store_memory': 11220602060.0, 'node:10.244.24.201': 1.0, 'memory': 246362272563.0, 'CPU': 28.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"  Cluster Information:\")\n",
    "for i, node in enumerate(nodes):\n",
    "    node_resources = node.get('Resources', {})\n",
    "    print(f\"   Node {i+1}: {node_resources}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def generate_data_chunk(chunk_id, chunk_size, num_features):\n",
    "    import numpy as np\n",
    "    np.random.seed(chunk_id)\n",
    "    data = []\n",
    "    for i in range(chunk_size):\n",
    "        features = np.random.normal(0, 1, num_features)\n",
    "        features[1] = features[0] * 0.5 + np.random.normal(0, 0.5)\n",
    "        features[2] = features[0] * 0.3 + features[1] * 0.2 + np.random.normal(0, 0.8)\n",
    "        signal = (features[0] * 0.5 + \n",
    "                 features[1] * 0.3 + \n",
    "                 features[2] * 0.2 + \n",
    "                 np.sum(features[3:6]) * 0.1)\n",
    "        target = 1 if signal + np.random.normal(0, 0.5) > 0 else 0\n",
    "        \n",
    "        # Create dictionary instead of list\n",
    "        row = {\"ID\": chunk_id * chunk_size + i}\n",
    "        for j, feature_val in enumerate(features):\n",
    "            row[f\"FEATURE_{j}\"] = float(feature_val)\n",
    "        row[\"TARGET\"] = int(target)\n",
    "        \n",
    "        data.append(row)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_large_dataset_and_dump_into_snowflake(num_rows=1_000_000, num_features=20):\n",
    "    \"\"\"\n",
    "    Create a large synthetic dataset using Ray for parallel data generation\n",
    "    \"\"\"\n",
    "    import pyarrow as pa\n",
    "    print(f\"Generating {num_rows:,} rows with {num_features} features for classification...\")\n",
    "    # Generate data in parallel using Ray\n",
    "    num_chunks = 50  # Distribute data generation\n",
    "    chunk_size = num_rows // num_chunks\n",
    "    print(f\"   Using {num_chunks} parallel data generation tasks...\")\n",
    "    # Generate data chunks in parallel\n",
    "    chunk_futures = [\n",
    "        generate_data_chunk.remote(i, chunk_size, num_features) \n",
    "        for i in range(num_chunks)\n",
    "    ]\n",
    "    chunks = ray.get(chunk_futures)\n",
    "    # Flatten data\n",
    "    all_data = []\n",
    "    for chunk in chunks:\n",
    "        all_data.extend(chunk)\n",
    "    print(f\"Generated {len(all_data):,} data points\")\n",
    "    # Define Arrow schema\n",
    "    fields = [pa.field(\"ID\", pa.int64())]\n",
    "    for i in range(num_features):\n",
    "        fields.append(pa.field(f\"FEATURE_{i}\", pa.float64()))\n",
    "    fields.append(pa.field(\"TARGET\", pa.int32()))\n",
    "    schema = pa.schema(fields)\n",
    "    \n",
    "    # Create Arrow Table with schema\n",
    "    print(\"🔧 Creating Arrow Table with schema...\")\n",
    "    table = pa.Table.from_pylist(all_data, schema=schema)\n",
    "    \n",
    "    # Create Ray Dataset from Arrow Table\n",
    "    print(\"🔧 Creating Ray Dataset...\")\n",
    "    ray_ds = ray.data.from_arrow(table)\n",
    "    \n",
    "    print(f\"✅ Created Ray Dataset with schema: {ray_ds.schema()}\")\n",
    "    return ray_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1,000,000 rows with 20 features for classification...\n",
      "   Using 50 parallel data generation tasks...\n",
      "Generated 1,000,000 data points\n",
      "🔧 Creating Arrow Table with schema...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 02:47:51,589\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Creating Ray Dataset...\n",
      "✅ Created Ray Dataset with schema: Column      Type\n",
      "------      ----\n",
      "ID          int64\n",
      "FEATURE_0   double\n",
      "FEATURE_1   double\n",
      "FEATURE_2   double\n",
      "FEATURE_3   double\n",
      "FEATURE_4   double\n",
      "FEATURE_5   double\n",
      "FEATURE_6   double\n",
      "FEATURE_7   double\n",
      "FEATURE_8   double\n",
      "FEATURE_9   double\n",
      "FEATURE_10  double\n",
      "FEATURE_11  double\n",
      "FEATURE_12  double\n",
      "FEATURE_13  double\n",
      "FEATURE_14  double\n",
      "FEATURE_15  double\n",
      "FEATURE_16  double\n",
      "FEATURE_17  double\n",
      "FEATURE_18  double\n",
      "FEATURE_19  double\n",
      "TARGET      int32\n"
     ]
    }
   ],
   "source": [
    "ray_ds = create_large_dataset_and_dump_into_snowflake(1_000_000, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 02:47:52,328\tINFO dataset.py:2704 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2025-07-08 02:47:52,331\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /raylogs/ray/session_2025-07-07_17-26-31_254142_14/logs/ray-data\n",
      "2025-07-08 02:47:52,332\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> LimitOperator[limit=1]\n",
      "Running 0: 0.00 row [00:00, ? row/s]\n",
      "                                                                              \n",
      "✔️  Dataset execution finished in 0.02 seconds: : 1.00 row [00:00, 61.1 row/s]\n",
      "\n",
      "- limit=1 1: 0.00 row [00:00, ? row/s]\u001b[A\n",
      "- limit=1: Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 172.0B object store: : 0.00 row [00:00, ? row/s]\u001b[A\n",
      "- limit=1: Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 172.0B object store: : 1.00 row [00:00, 58.3 row/s]\u001b[A\n",
      "- limit=1: Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 172.0B object store: : 1.00 row [00:00, 54.7 row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': 0, 'FEATURE_0': 1.764052345967664, 'FEATURE_1': -0.39446873493320733, 'FEATURE_2': 0.9732168331559461, 'FEATURE_3': 2.240893199201458, 'FEATURE_4': 1.8675579901499675, 'FEATURE_5': -0.977277879876411, 'FEATURE_6': 0.9500884175255894, 'FEATURE_7': -0.1513572082976979, 'FEATURE_8': -0.10321885179355784, 'FEATURE_9': 0.41059850193837233, 'FEATURE_10': 0.144043571160878, 'FEATURE_11': 1.454273506962975, 'FEATURE_12': 0.7610377251469934, 'FEATURE_13': 0.12167501649282841, 'FEATURE_14': 0.44386323274542566, 'FEATURE_15': 0.33367432737426683, 'FEATURE_16': 1.4940790731576061, 'FEATURE_17': -0.20515826376580087, 'FEATURE_18': 0.31306770165090136, 'FEATURE_19': -0.8540957393017248, 'TARGET': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ray_ds.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray_ds.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper utility to write ray dataset to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data import Datasink\n",
    "from ray.data import Datasource\n",
    "from ray.data._internal.execution.interfaces import TaskContext\n",
    "from ray.data.block import Block, BlockAccessor\n",
    "from ray.data.datasource import Reader, WriteResult\n",
    "from snowflake.connector import connect\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from typing import Optional, Iterable\n",
    "from ray.data._internal.remote_fn import cached_remote_fn\n",
    "class SnowflakeTableDatasink(Datasink):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        table_name: str,\n",
    "        database: Optional[str] = None,\n",
    "        schema: Optional[str] = None,\n",
    "        auto_create_table: bool = False,\n",
    "        override: bool = False,\n",
    "    ):\n",
    "        self._table_name = table_name\n",
    "        self._database = database\n",
    "        self._schema = schema\n",
    "        self._override = override\n",
    "        self._auto_create_table = auto_create_table\n",
    "\n",
    "    def on_write_start(self):\n",
    "        if not self._auto_create_table:\n",
    "            # table should already exist\n",
    "            session = get_session()\n",
    "            tables = session.sql(\n",
    "                f\"SHOW TABLES LIKE '{self._table_name}' IN {self._database}.{self._schema}\"\n",
    "            ).collect()\n",
    "            if not tables:\n",
    "                raise ValueError(\n",
    "                    f\"Table {self._database}.{self._schema}.{self._table_name} does not exist.\"\n",
    "                )\n",
    "\n",
    "        if self._override:\n",
    "            logger.warning(\n",
    "                f\"Overriding table {self._database}.{self._schema}.{self._table_name}\"\n",
    "            )\n",
    "            session = get_session()\n",
    "            session.sql(\n",
    "                f\"DROP TABLE IF EXISTS {self._database}.{self._schema}.{self._table_name}\"\n",
    "            ).collect()\n",
    "\n",
    "    def write(self, blocks: Iterable[Block], ctx: TaskContext) -> None:\n",
    "        # Write the blocks to the Snowflake table\n",
    "        def write_single_block(block: Block):\n",
    "            session = get_session()\n",
    "            pandas_block = BlockAccessor.for_block(block).to_pandas()\n",
    "            if not pandas_block.empty:\n",
    "                session.write_pandas(\n",
    "                    df=pandas_block,\n",
    "                    table_name=self._table_name,\n",
    "                    database=self._database.strip('\"'),\n",
    "                    schema=self._schema.strip('\"'),\n",
    "                    auto_create_table=self._auto_create_table,\n",
    "                    overwrite=False,  # append to the existing table as we are using multi-process to write the table.\n",
    "                )\n",
    "\n",
    "        # We need cache_remote_fn because there is a circular dependency issue with the ray.remote\n",
    "        # this is from one of the existing ray datasink for writing to BigQuery table.\n",
    "        remote_write_single_block = cached_remote_fn(write_single_block)\n",
    "        # Write the block to the Snowflake table\n",
    "        ray.get([remote_write_single_block.remote(block) for block in blocks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='SPARK_MLLIB_SAMPLE_DATASET successfully dropped.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"drop table if exists SPARK_MLLIB_SAMPLE_DATASET\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasink = SnowflakeTableDatasink(\n",
    "    table_name=\"SPARK_MLLIB_SAMPLE_DATASET\",\n",
    "    database=session.get_current_database(),\n",
    "    schema=session.get_current_schema(),\n",
    "    auto_create_table=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 02:47:52,528\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /raylogs/ray/session_2025-07-07_17-26-31_254142_14/logs/ray-data\n",
      "2025-07-08 02:47:52,528\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[Write]\n",
      "Running 0: 0.00 row [00:00, ? row/s]\n",
      "- Write 1: 0.00 row [00:00, ? row/s]\u001b[A\n",
      "- Write: Tasks: 1; Queued blocks: 0; Resources: 1.0 CPU, 256.0MB object store: : 0.00 row [00:01, ? row/s]\u001b[A\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:01, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:02, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:03, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:04, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:05, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:06, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:07, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:08, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:09, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:10, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:11, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:12, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:13, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:14, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:15, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:16, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:17, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:18, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:19, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:20, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:21, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:22, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:23, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:24, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:25, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:26, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:27, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:28, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:29, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:30, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:31, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/112 CPU, 256.0MB/20.9GB object store: : 0.00 row [00:32, ? row/s]\n",
      "                                                                                                                  \n",
      "✔️  Dataset execution finished in 34.14 seconds: 100%|██████████| 1.00/1.00 [00:34<00:00, 34.1s/ row]     \n",
      "\n",
      "- Write: Tasks: 1; Queued blocks: 0; Resources: 1.0 CPU, 256.0MB object store: : 0.00 row [00:34, ? row/s]\u001b[A\n",
      "- Write: Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 160.0B object store: : 0.00 row [00:34, ? row/s] \u001b[A\n",
      "- Write: Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 160.0B object store: : 1.00 row [00:34, 34.1s/ row]\u001b[A\n",
      "- Write: Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 160.0B object store: : 1.00 row [00:34, 34.1s/ row]\u001b[A\n",
      "- Write: Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 160.0B object store: : 1.00 row [00:34, 34.1s/ row]\n",
      "2025-07-08 02:48:26,714\tINFO dataset.py:3969 -- Data sink SnowflakeTable finished. 1000000 rows and 164.0MB data written.\n"
     ]
    }
   ],
   "source": [
    "ray_ds.write_datasink(datasink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def trigger_gc():\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "ray.get([trigger_gc.remote() for _ in range(len(ray.nodes()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
