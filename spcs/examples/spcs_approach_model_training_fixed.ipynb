{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# RayDP - Distributed Spark MLLib Model Training\n",
    "\n",
    "This notebook demonstrates how to properly distribute RayDP workloads across Ray cluster nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-08 03:05:41,941\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-07-08 03:05:42,431\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray version: 2.42.0\n",
      "RayDP version: 1.6.2\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import ray\n",
    "import raydp\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.insert(0, '/home/artifacts/')\n",
    "\n",
    "from snowflake.ml.data.data_connector import DataConnector\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, when, round as spark_round\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import snowflake.connector\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "print(f\"Ray version: {ray.__version__}\")\n",
    "print(f\"RayDP version: {raydp.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distributed_ray_dataset(train_snowdf, nodes):\n",
    "    print(\"Converting to Ray dataset...\")\n",
    "    ray_ds = DataConnector.from_dataframe(train_snowdf).to_ray_dataset()\n",
    "    num_partitions = len(nodes) * 4  # 4 partitions per node for good distribution\n",
    "    print(f\"Distributing data across {num_partitions} partitions (was 1 partition)\")\n",
    "    ray_ds_distributed = ray_ds.repartition(num_partitions)\n",
    "    ray_ds_arrow = ray_ds_distributed.map_batches(lambda b: b, batch_format=\"pyarrow\")\n",
    "    return ray_ds_arrow\n",
    "\n",
    "def get_distributed_spark_config(cluster_resources, nodes):\n",
    "    total_cpus = int(cluster_resources.get('CPU', 24))\n",
    "    total_memory_gb = cluster_resources.get('memory', 72 * 1024**3) / (1024**3)\n",
    "    num_nodes = len(nodes)\n",
    "    worker_nodes = [n for n in nodes if 'node:__internal_head__' not in n.get('Resources', {})]\n",
    "    num_worker_nodes = len(worker_nodes)\n",
    "    print(f\"Enhanced Cluster Analysis:\")\n",
    "    print(f\"  Total nodes: {num_nodes}\")\n",
    "    print(f\"  Worker nodes: {num_worker_nodes}\")\n",
    "    print(f\"  Total CPUs: {total_cpus}\")\n",
    "    print(f\"  Total Memory: {total_memory_gb:.1f} GB\")\n",
    "    num_executors = num_nodes * 2  # 2 executors per node instead of 1\n",
    "    # Calculate cores per executor (leave some for Ray/OS)\n",
    "    cpus_per_node = total_cpus // num_nodes\n",
    "    reserved_cpus_per_node = 2  # Reserve for Ray and OS\n",
    "    available_cpus_per_node = max(1, cpus_per_node - reserved_cpus_per_node)\n",
    "    executor_cores = max(1, available_cpus_per_node // 2)  # 2 executors per node\n",
    "    # Calculate memory per executor\n",
    "    memory_per_node = total_memory_gb / num_nodes\n",
    "    reserved_memory_per_node = 8  # Reserve for Ray and OS\n",
    "    available_memory_per_node = max(4, memory_per_node - reserved_memory_per_node)\n",
    "    executor_memory_gb = max(2, int(available_memory_per_node // 3))  # Conservative\n",
    "    driver_memory_gb = max(2, int(available_memory_per_node // 3))\n",
    "    config = {\n",
    "        'num_executors': num_executors,\n",
    "        'executor_cores': executor_cores,\n",
    "        'executor_memory': f\"{executor_memory_gb}g\",\n",
    "        'driver_memory': f\"{driver_memory_gb}g\"\n",
    "    }\n",
    "    \n",
    "    print(f\"Optimized Spark Configuration:\")\n",
    "    print(f\"  Executors: {config['num_executors']} (was {num_nodes})\")\n",
    "    print(f\"  Executor cores: {config['executor_cores']} per executor\")\n",
    "    print(f\"  Executor memory: {config['executor_memory']}\")\n",
    "    print(f\"  Driver memory: {config['driver_memory']}\")\n",
    "    \n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distributed_spark_configs():\n",
    "    return {\n",
    "        # Use Java serializer for better compatibility\n",
    "        \"spark.serializer\": \"org.apache.spark.serializer.JavaSerializer\",\n",
    "        \n",
    "        # Adaptive query execution\n",
    "        \"spark.sql.adaptive.enabled\": \"true\",\n",
    "        \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\",\n",
    "        \"spark.sql.adaptive.advisoryPartitionSizeInBytes\": \"128MB\",\n",
    "        \n",
    "        # Network and timeout settings\n",
    "        \"spark.network.timeout\": \"800s\",\n",
    "        \"spark.executor.heartbeatInterval\": \"60s\",\n",
    "        \"spark.storage.blockManagerSlaveTimeoutMs\": \"600s\",\n",
    "        \n",
    "        # CRITICAL: Higher parallelism for better distribution\n",
    "        \"spark.sql.shuffle.partitions\": \"400\",  # Higher for better distribution\n",
    "        \"spark.default.parallelism\": \"400\",\n",
    "        \n",
    "        # Memory and execution settings\n",
    "        \"spark.sql.execution.arrow.pyspark.enabled\": \"true\",\n",
    "        \"spark.sql.execution.arrow.maxRecordsPerBatch\": \"10000\",\n",
    "        \"spark.task.maxFailures\": \"3\",\n",
    "        \"spark.ml.tree.maxMemoryInMB\": \"1024\",\n",
    "        \n",
    "        # Compression settings\n",
    "        \"spark.rdd.compress\": \"true\",\n",
    "        \"spark.io.compression.codec\": \"snappy\",\n",
    "        \"spark.broadcast.blockSize\": \"4m\",\n",
    "        \"spark.sql.autoBroadcastJoinThreshold\": \"10MB\",\n",
    "        \n",
    "        # RayDP specific settings\n",
    "        \"spark.sql.adaptive.localShuffleReader.enabled\": \"false\",\n",
    "        \"spark.scheduler.blacklist.enabled\": \"false\",\n",
    "        \n",
    "        # CRITICAL: Force executor placement across nodes\n",
    "        \"spark.locality.wait\": \"1s\",\n",
    "        \"spark.locality.wait.process\": \"1s\",\n",
    "        \"spark.locality.wait.node\": \"1s\",\n",
    "        \"spark.locality.wait.rack\": \"1s\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_ray_logger() -> None:\n",
    "    #Configure Ray logging\n",
    "    ray_logger = logging.getLogger(\"ray\")\n",
    "    ray_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "    data_logger = logging.getLogger(\"ray.data\")\n",
    "    data_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "    #Configure root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "    #Configure Ray's data context\n",
    "    context = ray.data.DataContext.get_current()\n",
    "    context.execution_options.verbose_progress = False\n",
    "    context.enable_operator_progress_bars = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 03:05:52,689\tINFO worker.py:1654 -- Connecting to existing Ray cluster at address: raydpheadservice:6379...\n",
      "2025-07-08 03:05:52,792\tINFO worker.py:1832 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m10.244.24.9:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== APPLYING CRITICAL FIXES FOR DISTRIBUTED RAYDP ===\\n\n",
      "1. Connecting to Ray cluster...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-08 03:05:52,839 I 31347 31347] logging.cc:293: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n2. Analyzing cluster resources...\n",
      "Found 4 nodes with 112.0 total CPUs\n",
      "\\n3. Calculating distributed Spark configuration...\n",
      "Enhanced Cluster Analysis:\n",
      "  Total nodes: 4\n",
      "  Worker nodes: 3\n",
      "  Total CPUs: 112\n",
      "  Total Memory: 908.1 GB\n",
      "Optimized Spark Configuration:\n",
      "  Executors: 8 (was 4)\n",
      "  Executor cores: 13 per executor\n",
      "  Executor memory: 73g\n",
      "  Driver memory: 73g\n",
      "\\n4. Initializing RayDP with distributed configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/08 03:05:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/08 03:05:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RayDP initialized with 8 executors\n",
      "\\n5. Loading and distributing data...\n",
      "Converting to Ray dataset...\n",
      "Distributing data across 16 partitions (was 1 partition)\n",
      "\\n6. Converting to Spark DataFrame with proper partitioning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 0: 0.00 row [00:00, ? row/s]\n",
      "\n",
      "Split Repartition 2:   0%|          | 0.00/1.00 [00:00<?, ? row/s]\u001b[A\u001b[A\n",
      "\n",
      "  *- Split Repartition:   0%|          | 0.00/1.00 [00:01<?, ? row/s]\u001b[A\u001b[A\n",
      "\n",
      "  *- Split Repartition:   0%|          | 0.00/438k [00:01<?, ? row/s]\u001b[A\u001b[A\n",
      "\n",
      "  *- Split Repartition: 100%|██████████| 438k/438k [00:01<00:00, 339k row/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                                               \n",
      "\n",
      "✔️  Dataset execution finished in 2.58 seconds: 100%|██████████| 1.00/1.00 [00:02<00:00, 2.58s/ row]\n",
      "\n",
      "\n",
      "  *- Split Repartition: 100%|██████████| 438k/438k [00:02<00:00, 339k row/s]\u001b[A\u001b[A\n",
      "\n",
      "  *- Split Repartition:  63%|██████▎   | 438k/700k [00:02<00:00, 339k row/s]\u001b[A\u001b[A\n",
      "\n",
      "  *- Split Repartition: 100%|██████████| 700k/700k [00:02<00:00, 260k row/s]\u001b[A\u001b[A\n",
      "\n",
      "  *- Split Repartition: 100%|██████████| 700k/700k [00:02<00:00, 260k row/s]\u001b[A\u001b[A\n",
      "\n",
      "  *- Split Repartition: 100%|██████████| 700k/700k [00:02<00:00, 271k row/s]\n",
      "Running 0: 0.00 row [00:00, ? row/s]\n",
      "\n",
      "                                                                                                    \n",
      "\n",
      "✔️  Dataset execution finished in 0.91 seconds: 100%|██████████| 700k/700k [00:00<00:00, 771k row/s]\n",
      "\n",
      "\n",
      "Split Repartition 2:   0%|          | 0.00/1.00 [00:00<?, ? row/s]\u001b[A\u001b[A\n",
      "\n",
      "  *- Split Repartition:   0%|          | 0.00/1.00 [00:00<?, ? row/s]\u001b[A\u001b[A\n",
      "\n",
      "  *- Split Repartition:   0%|          | 0.00/700k [00:00<?, ? row/s]\u001b[A\u001b[A\n",
      "\n",
      "  *- Split Repartition: 100%|██████████| 700k/700k [00:00<00:00, 770k row/s]\u001b[A\u001b[A\n",
      "\n",
      "  *- Split Repartition: 100%|██████████| 700k/700k [00:00<00:00, 770k row/s]\u001b[A\u001b[A\n",
      "\n",
      "  *- Split Repartition: 100%|██████████| 700k/700k [00:00<00:00, 768k row/s]\n",
      "[Stage 2:=======================================>               (74 + 26) / 104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataFrame created with 104 partitions, 700,465 rows\n",
      "\\n=== SETUP COMPLETE - READY FOR DISTRIBUTED TRAINING ===\n",
      "Before training, please verify in Ray Dashboard that:\n",
      "- All nodes show activity in the Ray Dashboard\n",
      "- Spark UI shows executors across multiple hosts\n",
      "- Data is properly partitioned and cached\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Step 1: Connect to Ray cluster\n",
    "print(\"1. Connecting to Ray cluster...\")\n",
    "cli = ray.init(address=\"raydpheadservice:6379\", ignore_reinit_error=True, log_to_driver=False)\n",
    "configure_ray_logger()\n",
    "\n",
    "# Step 2: Analyze cluster resources\n",
    "print(\"\\\\n2. Analyzing cluster resources...\")\n",
    "cluster_resources = ray.cluster_resources()\n",
    "nodes = ray.nodes()\n",
    "print(f\"Found {len(nodes)} nodes with {cluster_resources.get('CPU', 0)} total CPUs\")\n",
    "\n",
    "# Step 3: Get distributed Spark configuration\n",
    "print(\"\\\\n3. Calculating distributed Spark configuration...\")\n",
    "spark_config = get_distributed_spark_config(cluster_resources, nodes)\n",
    "spark_configs = get_distributed_spark_configs()\n",
    "spark_configs[\"spark.driver.memory\"] = spark_config['driver_memory']\n",
    "\n",
    "# Step 4: Initialize RayDP with distributed config\n",
    "print(\"\\\\n4. Initializing RayDP with distributed configuration...\")\n",
    "spark = raydp.init_spark(\n",
    "    app_name=\"RayDP_Distributed_MLLib_Training\",\n",
    "    num_executors=spark_config['num_executors'],\n",
    "    executor_cores=spark_config['executor_cores'],\n",
    "    executor_memory=spark_config['executor_memory'],\n",
    "    configs=spark_configs\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(f\"RayDP initialized with {spark_config['num_executors']} executors\")\n",
    "\n",
    "# Step 5: Load and prepare data properly\n",
    "print(\"\\\\n5. Loading and distributing data...\")\n",
    "def get_session():\n",
    "    def get_token():\n",
    "        return open('/snowflake/session/token', 'r').read()\n",
    "    \n",
    "    def connection():\n",
    "        creds = {\n",
    "            'host': os.getenv('SNOWFLAKE_HOST'),\n",
    "            'port': os.getenv('SNOWFLAKE_PORT'),\n",
    "            'protocol': \"https\",\n",
    "            'account': os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "            'authenticator': \"oauth\",\n",
    "            'token': get_token(),\n",
    "            'warehouse': \"LARGE_WH\",\n",
    "            'database': os.getenv('SNOWFLAKE_DATABASE'),\n",
    "            'schema': os.getenv('SNOWFLAKE_SCHEMA'),\n",
    "            'client_session_keep_alive': True\n",
    "        }\n",
    "        return snowflake.connector.connect(**creds)\n",
    "    \n",
    "    return Session.builder.configs({\"connection\": connection()}).create()\n",
    "\n",
    "session = get_session()\n",
    "train_snowdf = session.table(\"TRAIN_SPARK_MLLIB_DATASET\")\n",
    "\n",
    "ray_ds_distributed = create_distributed_ray_dataset(train_snowdf, nodes)\n",
    "\n",
    "print(\"\\\\n6. Converting to Spark DataFrame with proper partitioning...\")\n",
    "spark_partitions = spark_config['num_executors'] * spark_config['executor_cores']\n",
    "df = ray_ds_distributed.to_spark(spark).repartition(spark_partitions)\n",
    "df.cache()  # Cache across executors\n",
    "row_count = df.count()  # Trigger caching\n",
    "print(f\"✅ DataFrame created with {df.rdd.getNumPartitions()} partitions, {row_count:,} rows\")\n",
    "\n",
    "print(\"\\\\n=== SETUP COMPLETE - READY FOR DISTRIBUTED TRAINING ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n8. Running distributed training...\n",
      "=== STARTING DISTRIBUTED TRAINING ===\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 560,277 rows\n",
      "Test set: 140,188 rows\n",
      "\\n🚀 Starting distributed training...\n",
      "Monitor resource utilization in:\n",
      "- Ray Dashboard: Check CPU/memory usage across nodes\n",
      "- Spark UI: Check executor activity and task distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n✅ Training completed in 133.09 seconds\n",
      "\\nEvaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n📊 Model Performance:\n",
      "  AUC: 0.9033\n",
      "  Accuracy: 0.8169\n",
      "  Training time: 133.09s\n",
      "\\n=== FINAL VERIFICATION ===\n",
      "After training, verify that:\n",
      "1. All Ray worker nodes showed high CPU utilization during training\n",
      "2. Memory usage was distributed across all nodes\n",
      "3. Spark UI showed tasks executing on multiple hosts\n",
      "4. Training completed faster than single-node execution\n",
      "\\nFinal Results: {'auc': 0.9032950657088992, 'accuracy': 0.8168887493936714, 'training_time': 133.09215712547302}\n",
      "\\n9. Cleaning up resources...\n",
      "✅ Cleanup completed\n"
     ]
    }
   ],
   "source": [
    "def train_distributed_model(df, spark_config):\n",
    "    print(\"=== STARTING DISTRIBUTED TRAINING ===\\\\n\")\n",
    "    \n",
    "    # Feature preparation\n",
    "    feature_cols = [f\"FEATURE_{i}\" for i in range(20)]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"RAW_FEATURES\")\n",
    "    scaler = StandardScaler(inputCol=\"RAW_FEATURES\", outputCol=\"FEATURES\", withStd=True, withMean=True)\n",
    "    \n",
    "    # Model optimized for distributed training\n",
    "    rf = RandomForestClassifier(\n",
    "        labelCol=\"TARGET\",\n",
    "        featuresCol=\"FEATURES\",\n",
    "        numTrees=200,  # More trees for better distribution\n",
    "        maxDepth=12,\n",
    "        maxBins=32,\n",
    "        minInstancesPerNode=5,\n",
    "        subsamplingRate=0.8,  # Enable subsampling for better distribution\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "    \n",
    "    # Split and cache data\n",
    "    train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "    train_data.cache()\n",
    "    test_data.cache()\n",
    "    \n",
    "    train_count = train_data.count()\n",
    "    test_count = test_data.count()\n",
    "    print(f\"Training set: {train_count:,} rows\")\n",
    "    print(f\"Test set: {test_count:,} rows\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = pipeline.fit(train_data)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\\\n✅ Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\\\nEvaluating model...\")\n",
    "    predictions = model.transform(test_data)\n",
    "    \n",
    "    evaluator_auc = BinaryClassificationEvaluator(labelCol=\"TARGET\", metricName=\"areaUnderROC\")\n",
    "    evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"TARGET\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    \n",
    "    auc = evaluator_auc.evaluate(predictions)\n",
    "    accuracy = evaluator_acc.evaluate(predictions)\n",
    "    \n",
    "    print(f\"\\\\n📊 Model Performance:\")\n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Training time: {training_time:.2f}s\")\n",
    "    \n",
    "    return model, {'auc': auc, 'accuracy': accuracy, 'training_time': training_time}\n",
    "\n",
    "# Execute distributed training\n",
    "print(\"\\\\n8. Running distributed training...\")\n",
    "model, metrics = train_distributed_model(df, spark_config)\n",
    "\n",
    "# Final verification\n",
    "print(\"\\\\n=== FINAL VERIFICATION ===\")\n",
    "print(\"After training, verify that:\")\n",
    "print(\"1. All Ray worker nodes showed high CPU utilization during training\")\n",
    "print(\"2. Memory usage was distributed across all nodes\")\n",
    "print(\"3. Training completed faster than single-node execution\")\n",
    "print(f\"\\\\nFinal Results: {metrics}\")\n",
    "\n",
    "# Cleanup\n",
    "print(\"\\\\n9. Cleaning up resources...\")\n",
    "df.unpersist()\n",
    "session.close()\n",
    "print(\"✅ Cleanup completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
