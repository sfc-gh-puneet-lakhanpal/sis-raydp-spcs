{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# RayDP - Distributed Spark on Ray in Snowpark Container Services\n",
    "\n",
    "This notebook demonstrates how to use RayDP to run distributed Spark workloads on Ray cluster in Snowpark Container Services.\n",
    "\n",
    "Based on GCP Vertex AI documentation: https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/run-spark-on-ray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-27 23:19:43,224\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-06-27 23:19:43,798\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray version: 2.42.0\n",
      "RayDP version: 1.6.2\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import raydp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, avg, sum as spark_sum\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import os\n",
    "\n",
    "print(f\"Ray version: {ray.__version__}\")\n",
    "print(f\"RayDP version: {raydp.__version__}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Initialize Ray and Create Spark Session using RayDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 23:19:43,986\tINFO worker.py:1654 -- Connecting to existing Ray cluster at address: 10.244.178.11:6379...\n",
      "2025-06-27 23:19:43,998\tINFO worker.py:1832 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m10.244.178.11:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray cluster resources: {'CPU': 24.0, 'node:10.244.181.11': 1.0, 'object_store_memory': 34548841266.0, 'GPU': 4.0, 'memory': 77756481128.0, 'accelerator_type:A10G': 4.0, 'node:10.244.180.11': 1.0, 'node:10.244.178.11': 1.0, 'node:__internal_head__': 1.0, 'node:10.244.179.11': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-27 23:19:44,000 I 2049 2049] logging.cc:293: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n",
      "\u001b[33m(raylet, ip=10.244.179.11)\u001b[0m [2025-06-27 23:19:45,494 I 646 646] logging.cc:293: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n",
      "\u001b[36m(RayDPSparkMaster pid=1430)\u001b[0m [2025-06-27 23:19:48,555 I 2123 2151] gcs_client.cc:98: GcsClient has no Cluster ID set, and won't fetch from GCS.\n",
      "\u001b[36m(RayDPSparkMaster pid=1430)\u001b[0m [2025-06-27 23:19:48,683 I 2123 2151] gcs_client.cc:98: GcsClient has no Cluster ID set, and won't fetch from GCS.\n",
      "\u001b[36m(SparkExecutor pid=646, ip=10.244.179.11)\u001b[0m Setting default log level to \"WARN\".\n",
      "\u001b[36m(SparkExecutor pid=646, ip=10.244.179.11)\u001b[0m To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "\u001b[33m(raylet, ip=10.244.180.11)\u001b[0m [2025-06-27 23:19:50,253 I 408 412] logging.cc:293: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(SparkExecutor pid=646, ip=10.244.179.11)\u001b[0m 25/06/27 23:19:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\u001b[33m(raylet, ip=10.244.180.11)\u001b[0m [2025-06-27 23:19:55,521 I 497 505] gcs_client.cc:98: GcsClient has no Cluster ID set, and won't fetch from GCS.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "[Stage 0:>                                                          (0 + 0) / 2]\n",
      "[Stage 0:>                                                          (0 + 2) / 2]\n",
      "                                                                                \n",
      "\u001b[33m(raylet, ip=10.244.180.11)\u001b[0m [2025-06-27 23:19:55,542 I 495 504] logging.cc:293: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayDPSparkMaster pid=1433)\u001b[0m [2025-06-27 23:21:30,598 I 2262 2295] gcs_client.cc:98: GcsClient has no Cluster ID set, and won't fetch from GCS.\n",
      "\u001b[36m(RayDPSparkMaster pid=1433)\u001b[0m [2025-06-27 23:21:30,730 I 2262 2295] gcs_client.cc:98: GcsClient has no Cluster ID set, and won't fetch from GCS.\n",
      "\u001b[36m(RayDPSparkMaster pid=1433)\u001b[0m [2025-06-27 23:21:30,730 I 2262 2295] logging.cc:293: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n",
      "\u001b[33m(raylet, ip=10.244.181.11)\u001b[0m [2025-06-27 23:21:37,334 I 287 292] gcs_client.cc:98: GcsClient has no Cluster ID set, and won't fetch from GCS.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet, ip=10.244.181.11)\u001b[0m [2025-06-27 23:21:37,334 I 287 292] logging.cc:293: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet, ip=10.244.179.11)\u001b[0m [2025-06-27 23:24:58,031 I 817 817] logging.cc:293: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n",
      "\u001b[36m(RayDPSparkMaster pid=817, ip=10.244.179.11)\u001b[0m [2025-06-27 23:25:00,450 I 855 882] gcs_client.cc:98: GcsClient has no Cluster ID set, and won't fetch from GCS.\n",
      "\u001b[36m(RayDPSparkMaster pid=817, ip=10.244.179.11)\u001b[0m [2025-06-27 23:25:00,650 I 855 882] gcs_client.cc:98: GcsClient has no Cluster ID set, and won't fetch from GCS.\n",
      "\u001b[33m(raylet, ip=10.244.180.11)\u001b[0m [2025-06-27 23:25:04,926 I 759 763] logging.cc:293: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet, ip=10.244.180.11)\u001b[0m Collecting statsmodels\n",
      "\u001b[33m(raylet, ip=10.244.180.11)\u001b[0m   Downloading statsmodels-0.14.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "\u001b[33m(raylet, ip=10.244.180.11)\u001b[0m Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.26.4)\n",
      "\u001b[33m(raylet, ip=10.244.179.11)\u001b[0m [2025-06-27 23:25:04,957 I 1007 1012] gcs_client.cc:98: GcsClient has no Cluster ID set, and won't fetch from GCS.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet, ip=10.244.179.11)\u001b[0m [2025-06-27 23:25:04,957 I 1007 1012] logging.cc:293: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n",
      "\u001b[33m(raylet, ip=10.244.180.11)\u001b[0m Collecting patsy>=0.5.6 (from statsmodels)\n",
      "\u001b[33m(raylet, ip=10.244.180.11)\u001b[0m Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "\u001b[33m(raylet, ip=10.244.180.11)\u001b[0m Downloading statsmodels-0.14.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[33m(raylet, ip=10.244.180.11)\u001b[0m    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.8/10.8 MB 126.2 MB/s eta 0:00:00\n",
      "\u001b[33m(raylet, ip=10.244.180.11)\u001b[0m Installing collected packages: patsy, statsmodels\n",
      "\u001b[33m(raylet, ip=10.244.180.11)\u001b[0m \n",
      "\u001b[33m(raylet, ip=10.244.180.11)\u001b[0m Successfully installed patsy-1.0.1 statsmodels-0.14.4\n",
      "\u001b[33m(raylet, ip=10.244.180.11)\u001b[0m WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "\u001b[33m(raylet, ip=10.244.179.11)\u001b[0m Collecting statsmodels\n",
      "\u001b[33m(raylet, ip=10.244.179.11)\u001b[0m   Downloading statsmodels-0.14.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "\u001b[33m(raylet, ip=10.244.179.11)\u001b[0m Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.26.4)\n",
      "\u001b[33m(raylet, ip=10.244.179.11)\u001b[0m Collecting patsy>=0.5.6 (from statsmodels)\n",
      "\u001b[33m(raylet, ip=10.244.179.11)\u001b[0m Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "\u001b[33m(raylet, ip=10.244.179.11)\u001b[0m Downloading statsmodels-0.14.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[33m(raylet, ip=10.244.179.11)\u001b[0m    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.8/10.8 MB 129.1 MB/s eta 0:00:00\n",
      "\u001b[33m(raylet, ip=10.244.179.11)\u001b[0m Installing collected packages: patsy, statsmodels\n",
      "\u001b[33m(raylet, ip=10.244.179.11)\u001b[0m \n",
      "\u001b[33m(raylet, ip=10.244.179.11)\u001b[0m Successfully installed patsy-1.0.1 statsmodels-0.14.4\n",
      "\u001b[33m(raylet, ip=10.244.179.11)\u001b[0m WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ray if not already initialized\n",
    "ray.init(address=\"auto\", ignore_reinit_error=True)\n",
    "\n",
    "print(f\"Ray cluster resources: {ray.cluster_resources()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RayDP with Ray client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"first_name\":\"sue\",\"age\":32}', '{\"first_name\":\"li\",\"age\":3}', '{\"first_name\":\"bob\",\"age\":75}', '{\"first_name\":\"heo\",\"age\":13}']\n"
     ]
    }
   ],
   "source": [
    "@ray.remote\n",
    "class SparkExecutor:\n",
    "  import pyspark\n",
    "\n",
    "  spark: pyspark.sql.SparkSession = None\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    import ray\n",
    "    import raydp\n",
    "\n",
    "    self.spark = raydp.init_spark(\n",
    "      app_name=\"RayDP_Distributed_Spark_SPCS\",\n",
    "      num_executors=2,\n",
    "      executor_cores=2,\n",
    "      executor_memory=\"2G\",\n",
    "      configs={\n",
    "            \"spark.driver.memory\": \"4G\",  # Set driver memory here instead\n",
    "            \"spark.sql.adaptive.enabled\": \"true\",\n",
    "            \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\",\n",
    "            \"spark.sql.execution.arrow.pyspark.enabled\": \"true\"\n",
    "        }\n",
    "    )\n",
    "    print(f\"Spark version: {self.spark.version}\")\n",
    "    print(f\"Spark application ID: {self.spark.sparkContext.applicationId}\")\n",
    "    print(f\"Spark UI URL: {self.spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "  def get_data(self):\n",
    "    df = self.spark.createDataFrame(\n",
    "        [\n",
    "            (\"sue\", 32),\n",
    "            (\"li\", 3),\n",
    "            (\"bob\", 75),\n",
    "            (\"heo\", 13),\n",
    "        ],\n",
    "        [\"first_name\", \"age\"],\n",
    "    )\n",
    "    return df.toJSON().collect()\n",
    "\n",
    "  def stop_spark(self):\n",
    "    import raydp\n",
    "    raydp.stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SparkExecutor.remote()\n",
    "data = ray.get(s.get_data.remote())\n",
    "print(data)\n",
    "ray.get(s.stop_spark.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RayDP with Ray Job API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/27 23:21:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"first_name\":\"sue\",\"age\":32}', '{\"first_name\":\"li\",\"age\":3}', '{\"first_name\":\"bob\",\"age\":75}', '{\"first_name\":\"heo\",\"age\":13}']\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import raydp\n",
    "\n",
    "def get_data(spark: pyspark.sql.SparkSession):\n",
    "    df = spark.createDataFrame(\n",
    "        [\n",
    "            (\"sue\", 32),\n",
    "            (\"li\", 3),\n",
    "            (\"bob\", 75),\n",
    "            (\"heo\", 13),\n",
    "        ],\n",
    "        [\"first_name\", \"age\"],\n",
    "    )\n",
    "    return df.toJSON().collect()\n",
    "\n",
    "def stop_spark():\n",
    "    raydp.stop_spark()\n",
    "\n",
    "spark = raydp.init_spark(\n",
    "      app_name=\"RAYDP JOB EXAMPLE\",\n",
    "        num_executors=1,\n",
    "        executor_cores=1,\n",
    "        executor_memory=\"500M\",\n",
    "    )\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"Spark UI URL: {spark.sparkContext.uiWebUrl}\")\n",
    "print(get_data(spark))\n",
    "stop_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark Pandas UDF on Ray cluster on Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "import raydp\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_udf(spark: pyspark.sql.SparkSession):\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = spark.createDataFrame(pd.read_csv(\"https://www.datavis.ca/gallery/guerry/guerry.csv\"))\n",
    "    return df.select(func('Lottery','Literacy', 'Pop1831')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(StringType())\n",
    "def func(s1: pd.Series, s2: pd.Series, s3: pd.Series) -> str:\n",
    "    import numpy as np\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"statsmodels\"])\n",
    "    import statsmodels.api as sm\n",
    "    import statsmodels.formula.api as smf\n",
    "    \n",
    "    d = {'Lottery': s1, \n",
    "         'Literacy': s2,\n",
    "         'Pop1831': s3}\n",
    "    data = pd.DataFrame(d)\n",
    "\n",
    "    # Fit regression model (using the natural log of one of the regressors)\n",
    "    results = smf.ols('Lottery ~ Literacy + np.log(Pop1831)', data=data).fit()\n",
    "    return results.summary().as_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(func(Lottery, Literacy, Pop1831)='                      OLS Regression Results                     \\nDep. Variable:   ,Lottery         ,  R-squared:         ,   0.348\\nModel:           ,OLS             ,  Adj. R-squared:    ,   0.333\\nMethod:          ,Least Squares   ,  F-statistic:       ,   22.20\\nDate:            ,Fri, 27 Jun 2025,  Prob (F-statistic):,1.90e-08\\nTime:            ,23:25:14        ,  Log-Likelihood:    , -379.82\\nNo. Observations:,    86          ,  AIC:               ,   765.6\\nDf Residuals:    ,    83          ,  BIC:               ,   773.0\\nDf Model:        ,     2          ,                     ,        \\nCovariance Type: ,nonrobust       ,                     ,        \\n               ,   coef   , std err ,    t    ,P>|t| ,  [0.025 ,  0.975] \\nIntercept      ,  246.4341,   35.233,    6.995, 0.000,  176.358,  316.510\\nLiteracy       ,   -0.4889,    0.128,   -3.832, 0.000,   -0.743,   -0.235\\nnp.log(Pop1831),  -31.3114,    5.977,   -5.239, 0.000,  -43.199,  -19.424\\nOmnibus:      , 3.713,  Durbin-Watson:     ,   2.019\\nProb(Omnibus):, 0.156,  Jarque-Bera (JB):  ,   3.394\\nSkew:         ,-0.487,  Prob(JB):          ,   0.183\\nKurtosis:     , 3.003,  Cond. No.          ,    702.\\n\\nNotes:\\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark = raydp.init_spark(\n",
    "  app_name=\"RayDP UDF Example\",\n",
    "  num_executors=2,\n",
    "  executor_cores=4,\n",
    "  executor_memory=\"1500M\",\n",
    ")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"Spark UI URL: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_udf(spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
