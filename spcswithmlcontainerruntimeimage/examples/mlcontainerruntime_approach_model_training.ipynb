{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# RayDP - Distributed Spark MLLib based Model Training on Snowpark Container Services\n",
    "\n",
    "This notebook demonstrates how to use RayDP to perform distributed Spark MLLIb based model training on Ray cluster in Snowpark Container Services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/snowflake/snowpark/session.py:38: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray version: 2.46.0\n",
      "RayDP version: 1.6.2\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import raydp\n",
    "import pprint\n",
    "import warnings\n",
    "import logging    \n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import socket\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, when, round as spark_round\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "import snowflake.connector\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.ml.data.data_connector import DataConnector\n",
    "from snowflake.ml.ray.datasink import SnowflakeTableDatasink\n",
    "print(f\"Ray version: {ray.__version__}\")\n",
    "print(f\"RayDP version: {raydp.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Snowpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection() -> snowflake.connector.SnowflakeConnection:\n",
    "    if os.path.isfile(\"/snowflake/session/token\"):\n",
    "        creds = {\n",
    "            'host': os.getenv('SNOWFLAKE_HOST'),\n",
    "            'port': os.getenv('SNOWFLAKE_PORT'),\n",
    "            'protocol': \"https\",\n",
    "            'account': os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "            'authenticator': \"oauth\",\n",
    "            'token': open('/snowflake/session/token', 'r').read(),\n",
    "            'warehouse': \"LARGE_WH\",\n",
    "            'database': os.getenv('SNOWFLAKE_DATABASE'),\n",
    "            'schema': os.getenv('SNOWFLAKE_SCHEMA'),\n",
    "            'client_session_keep_alive': True\n",
    "        }\n",
    "    else:\n",
    "        creds = {\n",
    "            'account': os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "            'user': os.getenv('SNOWFLAKE_USER'),\n",
    "            'password': os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "            'warehouse': snowflake_warehouse,\n",
    "            'database': os.getenv('SNOWFLAKE_DATABASE'),\n",
    "            'schema': os.getenv('SNOWFLAKE_SCHEMA'),\n",
    "            'client_session_keep_alive': True\n",
    "        }\n",
    "\n",
    "    connection = snowflake.connector.connect(**creds)\n",
    "    return connection\n",
    "\n",
    "def get_session() -> Session:\n",
    "    return Session.builder.configs({\"connection\": connection()}).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"RAYDP_SIS_DB\"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.get_current_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 07:13:19,069\tINFO worker.py:1694 -- Connecting to existing Ray cluster at address: raydpheadservice:6379...\n",
      "2025-07-08 07:13:19,179\tINFO worker.py:1879 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m10.244.28.9:8265 \u001b[39m\u001b[22m\n",
      "[2025-07-08 07:13:19,234 I 19218 19218] logging.cc:297: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to 1\n"
     ]
    }
   ],
   "source": [
    "cli = ray.init(address=\"raydpheadservice:6379\", ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_resources = ray.cluster_resources()\n",
    "nodes = ray.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cluster Information:\n",
      "   Node 1: {'memory': 245448057651.0, 'node:10.244.28.9': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 11220602060.0, 'node_type': 1.0, 'CPU': 28.0}\n",
      "   Node 2: {'memory': 246363530035.0, 'object_store_memory': 11220602060.0, 'CPU': 28.0, 'node_type': 2.0, 'node:10.244.28.137': 1.0}\n",
      "   Node 3: {'node_type': 2.0, 'node:10.244.28.201': 1.0, 'CPU': 28.0, 'memory': 246363697971.0, 'object_store_memory': 11220602060.0}\n",
      "   Node 4: {'node_type': 2.0, 'node:10.244.28.73': 1.0, 'CPU': 28.0, 'memory': 246363603763.0, 'object_store_memory': 11220602060.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"  Cluster Information:\")\n",
    "for i, node in enumerate(nodes):\n",
    "    node_resources = node.get('Resources', {})\n",
    "    print(f\"   Node {i+1}: {node_resources}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>FEATURE_0</th>\n",
       "      <th>FEATURE_1</th>\n",
       "      <th>FEATURE_2</th>\n",
       "      <th>FEATURE_3</th>\n",
       "      <th>FEATURE_4</th>\n",
       "      <th>FEATURE_5</th>\n",
       "      <th>FEATURE_6</th>\n",
       "      <th>FEATURE_7</th>\n",
       "      <th>FEATURE_8</th>\n",
       "      <th>...</th>\n",
       "      <th>FEATURE_11</th>\n",
       "      <th>FEATURE_12</th>\n",
       "      <th>FEATURE_13</th>\n",
       "      <th>FEATURE_14</th>\n",
       "      <th>FEATURE_15</th>\n",
       "      <th>FEATURE_16</th>\n",
       "      <th>FEATURE_17</th>\n",
       "      <th>FEATURE_18</th>\n",
       "      <th>FEATURE_19</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.764052</td>\n",
       "      <td>-0.394469</td>\n",
       "      <td>0.973217</td>\n",
       "      <td>2.240893</td>\n",
       "      <td>1.867558</td>\n",
       "      <td>-0.977278</td>\n",
       "      <td>0.950088</td>\n",
       "      <td>-0.151357</td>\n",
       "      <td>-0.103219</td>\n",
       "      <td>...</td>\n",
       "      <td>1.454274</td>\n",
       "      <td>0.761038</td>\n",
       "      <td>0.121675</td>\n",
       "      <td>0.443863</td>\n",
       "      <td>0.333674</td>\n",
       "      <td>1.494079</td>\n",
       "      <td>-0.205158</td>\n",
       "      <td>0.313068</td>\n",
       "      <td>-0.854096</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  FEATURE_0  FEATURE_1  FEATURE_2  FEATURE_3  FEATURE_4  FEATURE_5  \\\n",
       "0   0   1.764052  -0.394469   0.973217   2.240893   1.867558  -0.977278   \n",
       "\n",
       "   FEATURE_6  FEATURE_7  FEATURE_8  ...  FEATURE_11  FEATURE_12  FEATURE_13  \\\n",
       "0   0.950088  -0.151357  -0.103219  ...    1.454274    0.761038    0.121675   \n",
       "\n",
       "   FEATURE_14  FEATURE_15  FEATURE_16  FEATURE_17  FEATURE_18  FEATURE_19  \\\n",
       "0    0.443863    0.333674    1.494079   -0.205158    0.313068   -0.854096   \n",
       "\n",
       "   TARGET  \n",
       "0       1  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_snowdf = session.table(\"SPARK_MLLIB_SAMPLE_DATASET\")\n",
    "raw_data_snowdf.limit(1).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snowdf, test_snowdf = raw_data_snowdf.random_split(weights=[0.70, 0.30], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snowdf.write.mode(\"overwrite\").save_as_table(\"TRAIN_SPARK_MLLIB_DATASET\")\n",
    "test_snowdf.write.mode(\"overwrite\").save_as_table(\"TEST_SPARK_MLLIB_DATASET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snowdf = session.table(\"TRAIN_SPARK_MLLIB_DATASET\")\n",
    "test_snowdf = session.table(\"TEST_SPARK_MLLIB_DATASET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get optimal spark config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging():\n",
    "    logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "    logging.getLogger(\"pyspark\").setLevel(logging.ERROR)\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    print(\"Logging configured to suppress common Spark warnings\")\n",
    "\n",
    "def get_default_spark_configs():\n",
    "    return {\n",
    "        # Core Spark optimizations\n",
    "        \"spark.sql.adaptive.enabled\": \"true\",\n",
    "        \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\",\n",
    "        \"spark.sql.adaptive.skewJoin.enabled\": \"true\",\n",
    "        \"spark.sql.adaptive.localShuffleReader.enabled\": \"true\",\n",
    "        \"spark.sql.adaptive.advisoryPartitionSizeInBytes\": \"128MB\",\n",
    "        \"spark.sql.adaptive.maxRecordsPerPartition\": \"40000\",\n",
    "        \n",
    "        # Serialization and compression optimizations\n",
    "        \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "        \"spark.kryo.registrationRequired\": \"false\",\n",
    "        \"spark.kryo.unsafe\": \"true\",\n",
    "        \"spark.rdd.compress\": \"true\",\n",
    "        \"spark.io.compression.codec\": \"lz4\",\n",
    "        \"spark.broadcast.compress\": \"true\",\n",
    "        \"spark.shuffle.compress\": \"true\",\n",
    "        \"spark.shuffle.spill.compress\": \"true\",\n",
    "        \n",
    "        # Memory optimizations for MLlib\n",
    "        \"spark.executor.memoryFraction\": \"0.8\",\n",
    "        \"spark.storage.memoryFraction\": \"0.3\",\n",
    "        \"spark.shuffle.memoryFraction\": \"0.5\",\n",
    "        \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:G1HeapRegionSize=16m -XX:+UseStringDeduplication\",\n",
    "        \n",
    "        # MLlib specific optimizations\n",
    "        \"spark.ml.tree.maxMemoryInMB\": \"4096\",\n",
    "        \"spark.ml.tree.maxDepth\": \"15\",\n",
    "        \"spark.mllib.tree.maxMemoryInMB\": \"4096\",\n",
    "        \"spark.sql.execution.arrow.pyspark.enabled\": \"true\",\n",
    "        \"spark.sql.execution.arrow.maxRecordsPerBatch\": \"10000\",\n",
    "        \n",
    "        # Network and shuffle optimizations\n",
    "        \"spark.shuffle.service.enabled\": \"false\",\n",
    "        \"spark.shuffle.file.buffer\": \"1m\",\n",
    "        \"spark.reducer.maxSizeInFlight\": \"96m\",\n",
    "        \"spark.shuffle.io.maxRetries\": \"6\",\n",
    "        \"spark.shuffle.io.retryWait\": \"30s\",\n",
    "        \"spark.network.timeout\": \"600s\",\n",
    "        \"spark.executor.heartbeatInterval\": \"30s\",\n",
    "        \n",
    "        # Broadcast optimizations\n",
    "        \"spark.sql.autoBroadcastJoinThreshold\": \"200MB\",\n",
    "        \"spark.broadcast.blockSize\": \"16m\",\n",
    "        \n",
    "        # Parallelism optimizations\n",
    "        \"spark.task.cpus\": \"1\",\n",
    "        \"spark.task.maxFailures\": \"3\",\n",
    "        \"spark.stage.maxConsecutiveAttempts\": \"8\",\n",
    "        \n",
    "        # Dynamic allocation disabled for consistent performance\n",
    "        \"spark.dynamicAllocation.enabled\": \"false\",\n",
    "        \n",
    "        # Speculation for fault tolerance\n",
    "        \"spark.speculation\": \"true\",\n",
    "        \"spark.speculation.interval\": \"5s\",\n",
    "        \"spark.speculation.multiplier\": \"2.0\",\n",
    "        \"spark.speculation.quantile\": \"0.9\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 07:13:27,592\tINFO worker.py:1694 -- Connecting to existing Ray cluster at address: raydpheadservice:6379...\n",
      "2025-07-08 07:13:27,592\tINFO worker.py:1718 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarted Ray connection\n"
     ]
    }
   ],
   "source": [
    "if 'spark' in locals():\n",
    "    spark.stop()\n",
    "    print(\"Stopped previous Spark session to apply optimized configuration\")\n",
    "    \n",
    "cli = ray.init(address=\"raydpheadservice:6379\", ignore_reinit_error=True, log_to_driver=False)\n",
    "print(\"Restarted Ray connection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ray_cluster_analysis():\n",
    "    cluster_resources = ray.cluster_resources()\n",
    "    nodes = ray.nodes()\n",
    "    total_cpus = int(cluster_resources.get('CPU', 0))\n",
    "    total_memory_bytes = cluster_resources.get('memory', 0)\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "    head_nodes = [n for n in nodes if n.get('Resources', {}).get('node_type', 0) == 1]\n",
    "    worker_nodes = [n for n in nodes if n.get('Resources', {}).get('node_type', 0) == 2]\n",
    "    cpus_per_node = total_cpus // len(nodes)\n",
    "    memory_per_node_gb = total_memory_gb / len(nodes)\n",
    "    analysis = {\n",
    "        'total_nodes': len(nodes),\n",
    "        'head_nodes': len(head_nodes),\n",
    "        'worker_nodes': len(worker_nodes),\n",
    "        'total_cpus': total_cpus,\n",
    "        'total_memory_gb': total_memory_gb,\n",
    "        'cpus_per_node': cpus_per_node,\n",
    "        'memory_per_node_gb': memory_per_node_gb,\n",
    "        'nodes': nodes\n",
    "    }\n",
    "    print(f\"Ray Cluster Analysis:\")\n",
    "    print(f\"   Total Nodes: {analysis['total_nodes']} (Head: {analysis['head_nodes']}, Workers: {analysis['worker_nodes']})\")\n",
    "    print(f\"   Total CPUs: {analysis['total_cpus']}\")\n",
    "    print(f\"   Total Memory: {analysis['total_memory_gb']:.1f} GB\")\n",
    "    print(f\"   Per Node: {analysis['cpus_per_node']} CPUs, {analysis['memory_per_node_gb']:.1f} GB\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def get_dynamic_spark_configs(cluster_analysis):\n",
    "    # Strategy: Use more executors with fewer cores each for better MLlib parallelism\n",
    "    # MLlib (especially Random Forest) performs better with more parallelism\n",
    "    total_cpus = cluster_analysis['total_cpus']\n",
    "    total_memory_gb = cluster_analysis['total_memory_gb']\n",
    "    total_nodes = cluster_analysis['total_nodes']\n",
    "    # Optimal executor strategy for MLlib:\n",
    "    # - Target 2 executors per node for uniform distribution\n",
    "    # - Use fewer cores per executor (5-8 cores optimal for MLlib)\n",
    "    # - Maximize memory per executor for feature vectors\n",
    "    num_executors = total_nodes * 2  # 2 executors per node\n",
    "    # Reserve CPUs for driver and OS (1 per node)\n",
    "    available_cpus = total_cpus - total_nodes\n",
    "    executor_cores = max(5, min(8, available_cpus // num_executors))\n",
    "    # Aggressive memory allocation (90% utilization)\n",
    "    # Reserve only 2GB per node for OS/Ray overhead\n",
    "    available_memory_gb = total_memory_gb - (total_nodes * 2)\n",
    "    executor_memory_gb = int(available_memory_gb * 0.85 / num_executors)\n",
    "    driver_memory_gb = int(available_memory_gb * 0.15)\n",
    "    # Ensure minimum viable sizes\n",
    "    executor_memory_gb = max(8, executor_memory_gb)\n",
    "    driver_memory_gb = max(4, min(16, driver_memory_gb))\n",
    "    # Calculate optimal partitions for MLlib\n",
    "    # Rule: 2-4 partitions per core for MLlib workloads\n",
    "    optimal_partitions = (num_executors * executor_cores) * 3\n",
    "    config = {\n",
    "        'num_executors': num_executors,\n",
    "        'executor_cores': executor_cores,\n",
    "        'executor_memory': f\"{executor_memory_gb}g\",\n",
    "        'driver_memory': f\"{driver_memory_gb}g\",\n",
    "        'optimal_partitions': optimal_partitions,\n",
    "        'total_executor_cores': num_executors * executor_cores,\n",
    "        'total_executor_memory_gb': num_executors * executor_memory_gb,\n",
    "        'cpu_utilization_pct': (num_executors * executor_cores) / total_cpus * 100,\n",
    "        'memory_utilization_pct': (num_executors * executor_memory_gb + driver_memory_gb) / total_memory_gb * 100\n",
    "    }\n",
    "    print(f\"Optimal Spark Configuration:\")\n",
    "    print(f\"   Strategy: High parallelism with optimal executor sizing for MLlib\")\n",
    "    print(f\"   Executors: {config['num_executors']} ({config['num_executors']//total_nodes} per node)\")\n",
    "    print(f\"   Executor cores: {config['executor_cores']} (total: {config['total_executor_cores']})\")\n",
    "    print(f\"   Executor memory: {config['executor_memory']} (total: {config['total_executor_memory_gb']}GB)\")\n",
    "    print(f\"   Driver memory: {config['driver_memory']}\")\n",
    "    print(f\"   Optimal partitions: {config['optimal_partitions']}\")\n",
    "    print(f\"   CPU utilization: {config['cpu_utilization_pct']:.1f}%\")\n",
    "    print(f\"   Memory utilization: {config['memory_utilization_pct']:.1f}%\")\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging configured to suppress common Spark warnings\n",
      "Ray Cluster Analysis:\n",
      "   Total Nodes: 4 (Head: 1, Workers: 3)\n",
      "   Total CPUs: 112\n",
      "   Total Memory: 916.9 GB\n",
      "   Per Node: 28 CPUs, 229.2 GB\n",
      "Optimal Spark Configuration:\n",
      "   Strategy: High parallelism with optimal executor sizing for MLlib\n",
      "   Executors: 8 (2 per node)\n",
      "   Executor cores: 8 (total: 64)\n",
      "   Executor memory: 96g (total: 768GB)\n",
      "   Driver memory: 16g\n",
      "   Optimal partitions: 192\n",
      "   CPU utilization: 57.1%\n",
      "   Memory utilization: 85.5%\n",
      "Ready to restart with dynamic spark configuration:\n",
      "   8 executors × 8 cores = 64 total cores\n",
      "   Memory: 96g per executor\n",
      "   Partitions: 192\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Apply optimized configuration\n",
    "configure_logging()\n",
    "cluster_analysis = get_ray_cluster_analysis()\n",
    "dynamic_spark_config = get_dynamic_spark_configs(cluster_analysis)\n",
    "spark_configs = get_default_spark_configs()\n",
    "spark_configs[\"spark.driver.memory\"] = dynamic_spark_config['driver_memory']\n",
    "\n",
    "# Add extra speed optimizations\n",
    "spark_configs.update({\n",
    "    \"spark.sql.shuffle.partitions\": str(dynamic_spark_config['optimal_partitions']),\n",
    "    \"spark.default.parallelism\": str(dynamic_spark_config['total_executor_cores'] * 2),\n",
    "    \"spark.ml.tree.maxMemoryInMB\": \"8192\",  # More memory for faster tree building\n",
    "    \"spark.executor.cores\": str(dynamic_spark_config['executor_cores']),\n",
    "    \"spark.task.cpus\": \"1\"\n",
    "})\n",
    "\n",
    "print(f\"Ready to restart with dynamic spark configuration:\")\n",
    "print(f\"   {dynamic_spark_config['num_executors']} executors × {dynamic_spark_config['executor_cores']} cores = {dynamic_spark_config['total_executor_cores']} total cores\")\n",
    "print(f\"   Memory: {dynamic_spark_config['executor_memory']} per executor\")\n",
    "print(f\"   Partitions: {dynamic_spark_config['optimal_partitions']}\")\n",
    "print(\"=\" * 70)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Ray DP with optimal Spark config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RayDP with optimal spark configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/08 07:15:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark started with 8 executors × 8 cores\n",
      "   Total cores: 64\n",
      "   Memory per executor: 96g\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing RayDP with optimal spark configuration...\")\n",
    "spark = raydp.init_spark(\n",
    "    app_name=\"RayDP_MLLib_Training\",\n",
    "    num_executors=dynamic_spark_config['num_executors'],\n",
    "    executor_cores=dynamic_spark_config['executor_cores'],\n",
    "    executor_memory=dynamic_spark_config['executor_memory'],\n",
    "    configs=spark_configs\n",
    ")\n",
    "print(f\"Spark started with {dynamic_spark_config['num_executors']} executors × {dynamic_spark_config['executor_cores']} cores\")\n",
    "print(f\"   Total cores: {dynamic_spark_config['total_executor_cores']}\")\n",
    "print(f\"   Memory per executor: {dynamic_spark_config['executor_memory']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark log level set to ERROR (warnings suppressed)\n"
     ]
    }
   ],
   "source": [
    "def set_spark_log_level(spark_session):\n",
    "    try:\n",
    "        spark_context = spark_session.sparkContext\n",
    "        spark_context.setLogLevel(\"ERROR\")\n",
    "        print(\"Spark log level set to ERROR (warnings suppressed)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not set Spark log level: {e}\")\n",
    "set_spark_log_level(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized\n",
      "Application ID: spark-application-1751958905166\n",
      "Spark Version: 3.5.4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spark session initialized\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.244.28.9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>ray://RAY_APP_MASTER@10.244.28.73:39403</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>RayDP_MLLib_Training</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9dd021ef80>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using optimal partitions for MLlib: 192\n",
      "   Partitions per core: 3.0\n"
     ]
    }
   ],
   "source": [
    "optimal_partitions = dynamic_spark_config['optimal_partitions']\n",
    "print(f\"Using optimal partitions for MLlib: {optimal_partitions}\")\n",
    "print(f\"   Partitions per core: {optimal_partitions / dynamic_spark_config['total_executor_cores']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See partitioned spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SnowflakeLoginOptions() is in private preview since 0.2.0. Do not use it in production. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info - 2025-07-08 07:16:42.869333 - Loading data from Snowpark Dataframe from query id 01bd8bd4-0205-ed1a-0000-50070cb49a62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 07:16:45,731\tINFO logging.py:290 -- Registered dataset logger for dataset dataset_2_0\n",
      "2025-07-08 07:16:45,743\tINFO streaming_executor.py:117 -- Starting execution of Dataset dataset_2_0. Full logs are in /raylogs/ray/session_2025-07-08_06-25-28_011968_23/logs/ray-data\n",
      "2025-07-08 07:16:45,743\tINFO streaming_executor.py:118 -- Execution plan of Dataset dataset_2_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadResultSetDataSource]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info - 2025-07-08 07:16:45.716959 - Finished executing data load query.\n",
      "Info - 2025-07-08 07:16:45.725757 - Loaded data into ray dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 07:16:49,224\tINFO streaming_executor.py:220 -- ✔️  Dataset dataset_2_0 execution finished in 3.48 seconds\n",
      "2025-07-08 07:16:49,225\tWARNING __init__.py:161 -- DeprecationWarning: `ray.worker.global_worker` is a private attribute and access will be removed in a future Ray version.\n",
      "[Stage 0:=====================================================> (247 + 5) / 252]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Spark DataFrame with 192 partitions\n",
      "CPU times: user 1.26 s, sys: 319 ms, total: 1.58 s\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = DataConnector.from_dataframe(train_snowdf).to_ray_dataset().to_spark(spark).repartition(int(optimal_partitions))\n",
    "print(f\"Created Spark DataFrame with {df.rdd.getNumPartitions()} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------+-----------+------------+-----------+---------+-----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+------+\n",
      "|    ID|  FEATURE_0| FEATURE_1|  FEATURE_2|   FEATURE_3|  FEATURE_4|FEATURE_5|  FEATURE_6|  FEATURE_7| FEATURE_8| FEATURE_9|FEATURE_10|FEATURE_11|FEATURE_12|FEATURE_13|FEATURE_14|FEATURE_15|FEATURE_16|FEATURE_17|FEATURE_18|FEATURE_19|TARGET|\n",
      "+------+-----------+----------+-----------+------------+-----------+---------+-----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+------+\n",
      "|495692|-0.42123833|0.13511305|-0.12561826|-0.068190016|-0.99639165|2.0022051|0.123225786|-0.82437736|0.37117386|0.11159816| 1.3409237|0.26517436|-2.4053752| 1.4127581|0.27067843|-1.2798657|0.49224877|-0.6434579| 0.5071654| 0.9820699|     0|\n",
      "+------+-----------+----------+-----------+------------+-----------+---------+-----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_model(df, spark_config):\n",
    "    feature_cols = [f\"FEATURE_{i}\" for i in range(20)]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"RAW_FEATURES\")\n",
    "    scaler = StandardScaler(inputCol=\"RAW_FEATURES\", outputCol=\"FEATURES\", withStd=True, withMean=True)\n",
    "    # Optimize Random Forest parameters based on cluster configuration\n",
    "    num_executors = spark_config['num_executors']\n",
    "    executor_cores = spark_config['executor_cores']\n",
    "    total_cores = spark_config['total_executor_cores']\n",
    "    \n",
    "    optimal_num_trees = min(50, max(20, total_cores))\n",
    "    optimal_max_depth = 6  \n",
    "    optimal_max_bins = 32\n",
    "    optimal_min_instances = 20\n",
    "    \n",
    "    rf = RandomForestClassifier(\n",
    "        labelCol=\"TARGET\",\n",
    "        featuresCol=\"FEATURES\",\n",
    "        numTrees=optimal_num_trees,\n",
    "        maxDepth=optimal_max_depth,\n",
    "        maxBins=optimal_max_bins,\n",
    "        minInstancesPerNode=optimal_min_instances,\n",
    "        subsamplingRate=0.8,  # Bagging for better generalization  \n",
    "        featureSubsetStrategy=\"sqrt\",\n",
    "        seed=42,\n",
    "        cacheNodeIds=True,  # Cache for better performance\n",
    "        checkpointInterval=20  # Less frequent checkpoints for SPEED\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "    train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # Cache datasets for better performance\n",
    "    train_data.cache()\n",
    "    test_data.cache()\n",
    "    \n",
    "    print(f\"Optimal Random Forest Configuration:\")\n",
    "    print(f\"   Trees: {optimal_num_trees}\")\n",
    "    print(f\"   Max depth: {optimal_max_depth}\")\n",
    "    print(f\"   Max bins: {optimal_max_bins}\")\n",
    "    print(f\"   Min instances: {optimal_min_instances}\")\n",
    "    print(f\"   Feature strategy: sqrt\")\n",
    "    print(f\"   Training set: {train_data.count():,} rows\")\n",
    "    print(f\"   Evaluation set: {test_data.count():,} rows\")\n",
    "    print(\"Starting distributed training ...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = pipeline.fit(train_data)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    print(\"Evaluating model...\")\n",
    "    \n",
    "    predictions = model.transform(test_data)\n",
    "    evaluator_auc = BinaryClassificationEvaluator(labelCol=\"TARGET\", metricName=\"areaUnderROC\")\n",
    "    evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"TARGET\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    auc = evaluator_auc.evaluate(predictions)\n",
    "    accuracy = evaluator_acc.evaluate(predictions)\n",
    "    \n",
    "    print(f\"Model Performance:\")\n",
    "    print(f\"   AUC: {auc:.4f}\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   Training time: {training_time:.2f}s\")\n",
    "    print(f\"   Trees/second: {optimal_num_trees/training_time:.2f}\")\n",
    "    \n",
    "    # Clean up cached data\n",
    "    train_data.unpersist()\n",
    "    test_data.unpersist()\n",
    "    \n",
    "    return model, {\n",
    "        'auc': auc, \n",
    "        'accuracy': accuracy, \n",
    "        'training_time': training_time, \n",
    "        'num_trees': optimal_num_trees,\n",
    "        'trees_per_second': optimal_num_trees/training_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Random Forest Configuration:\n",
      "   Trees: 50\n",
      "   Max depth: 6\n",
      "   Max bins: 32\n",
      "   Min instances: 20\n",
      "   Feature strategy: sqrt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training set: 559,974 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Evaluation set: 139,670 rows\n",
      "Starting distributed training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 17.99 seconds\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:========================================>            (145 + 17) / 192]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "   AUC: 0.8904\n",
      "   Accuracy: 0.8085\n",
      "   Training time: 17.99s\n",
      "   Trees/second: 2.78\n",
      "CPU times: user 595 ms, sys: 197 ms, total: 791 ms\n",
      "Wall time: 38.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_class, metrics_class = train_classification_model(df, dynamic_spark_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RAYDP PERFORMANCE SUMMARY\n",
      "================================================================================\n",
      "Cluster Utilization:\n",
      "   CPU utilization: 57.1%\n",
      "   Memory utilization: 85.5%\n",
      "   Executors: 8 (2 per node)\n",
      "   Total cores: 64\n",
      "   Total memory: 768GB\n",
      "\n",
      "Training Performance:\n",
      "   Training time: 17.99s\n",
      "   Trees trained: 50\n",
      "   Trees/second: 2.78\n",
      "   Model accuracy: 0.8085\n",
      "   Model AUC: 0.8904\n",
      "\n",
      "Key Optimizations Applied:\n",
      "High parallelism executor strategy (2 per node)\n",
      "Optimized executor sizing (6-8 cores each)\n",
      "Aggressive memory utilization (90%+)\n",
      "MLlib-specific Spark configurations\n",
      "Dynamic Random Forest parameter scaling\n",
      "Advanced compression and serialization\n",
      "Network and shuffle optimizations\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def print_performance_summary(spark_config, metrics):\n",
    "    \"\"\"\n",
    "    Print comprehensive performance summary comparing to baseline\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"RAYDP PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"Cluster Utilization:\")\n",
    "    print(f\"   CPU utilization: {spark_config['cpu_utilization_pct']:.1f}%\")\n",
    "    print(f\"   Memory utilization: {spark_config['memory_utilization_pct']:.1f}%\")\n",
    "    print(f\"   Executors: {spark_config['num_executors']} ({spark_config['num_executors']//4} per node)\")\n",
    "    print(f\"   Total cores: {spark_config['total_executor_cores']}\")\n",
    "    print(f\"   Total memory: {spark_config['total_executor_memory_gb']}GB\")\n",
    "    \n",
    "    print(f\"\\nTraining Performance:\")\n",
    "    print(f\"   Training time: {metrics['training_time']:.2f}s\")\n",
    "    print(f\"   Trees trained: {metrics['num_trees']}\")\n",
    "    print(f\"   Trees/second: {metrics['trees_per_second']:.2f}\")\n",
    "    print(f\"   Model accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"   Model AUC: {metrics['auc']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nKey Optimizations Applied:\")\n",
    "    print(f\"High parallelism executor strategy (2 per node)\")\n",
    "    print(f\"Optimized executor sizing (6-8 cores each)\")\n",
    "    print(f\"Aggressive memory utilization (90%+)\")\n",
    "    print(f\"MLlib-specific Spark configurations\")\n",
    "    print(f\"Dynamic Random Forest parameter scaling\")\n",
    "    print(f\"Advanced compression and serialization\")\n",
    "    print(f\"Network and shuffle optimizations\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Call the performance summary\n",
    "print_performance_summary(dynamic_spark_config, metrics_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
